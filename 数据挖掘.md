# 数据挖掘复习

![image-20230214163239602](数据挖掘.assets/image-20230214163239602.png)

![image-20230214163932362](数据挖掘.assets/image-20230214163932362.png)

![QQ图片20230214194230](数据挖掘.assets/QQ图片20230214194230.png)

## 朴素贝叶斯分类算法

### 课堂作业（重点）

![image-20230211131354825](数据挖掘.assets/image-20230211131354825.png)

![image-20230211131417408](数据挖掘.assets/image-20230211131417408.png)

步骤：

前提：10条已知数据，有一个条未知数据判断是否为好果

1.以结果（是/否）为分类计算概率：P（c=是）=4/10，P（c=否）=6/10

2.用第11条数据的三个特征算条件概率：在4个是好果中，有3个大小是大的果P（大小=大|c=是）=3/4，依次类推

3.结果为“是”和“否”的条件概率分别有3个，是一类的相乘，否一类的相乘，得出两个概率，谁的概率大谁就是对应的结果。

### 另一个实例

![image-20230211133831808](数据挖掘.assets/image-20230211133831808.png)

 12个数据，6个嫁，6个不嫁

p（嫁）=1/2,p (不嫁) =1/2

嫁6：p（帅|嫁）=1/2  p(性格好|嫁)=5/6 p(矮|嫁)=1/6 p(不上进|嫁)=1/6

不嫁6：p（帅|不嫁）=5/6  p(性格好|不嫁)=1/6  p(矮|不嫁)=1 p(不上进|不嫁)=1/2

p(嫁) * p(帅|嫁) *  p(性格好|嫁) * p(矮|嫁) * p(不上进|嫁)=5/(4 * 6 * 6*6)

p (不嫁) * p（帅|不嫁）* p(性格好|不嫁) * p(矮|不嫁) * p(不上进|不嫁)=5/(4 * 6 *6)

p(嫁)<p(不嫁)

所以不嫁

这不是简简单单！

![image-20230211133921957](数据挖掘.assets/image-20230211133921957.png)

## 关联规则挖掘——Apriori算法

### 课堂作业（重点）

![image-20230211135153828](数据挖掘.assets/image-20230211135153828.png)

![image-20230211135408571](数据挖掘.assets/image-20230211135408571.png)

最小支持度计数min_sup=3，表示会去除不满足次数不足3次的候选项

频繁k项集生成频繁k+1项集，就是排列组合。

### 例题

![image-20230211142401259](数据挖掘.assets/image-20230211142401259.png)

支持度=项集中的元素同时出现的次数/总的数量，比如一共有5条数据，烤鸭、面饼、面酱同时出现有2条。即2/5

置信度=项集中的元素同时出现的次数/关联规则前项出现的次数，比如烤鸭-》面饼、面酱，包含烤鸭的有3条数据，他们同时出现的数据有2条，即2/3

## 决策树分类算法

### 熵和信息增益

![image-20230211143755685](数据挖掘.assets/image-20230211143755685.png)

### ID3算法实例分析

![image-20230211143413355](数据挖掘.assets/image-20230211143413355.png)

![image-20230211143853525](数据挖掘.assets/image-20230211143853525.png)

![image-20230211143955739](数据挖掘.assets/image-20230211143955739.png)

![image-20230211144029101](数据挖掘.assets/image-20230211144029101.png)

![image-20230211144119741](数据挖掘.assets/image-20230211144119741.png)

![image-20230211144130796](数据挖掘.assets/image-20230211144130796.png)

![image-20230211144202148](数据挖掘.assets/image-20230211144202148.png)

0.9537为决策属性的熵

![image-20230211144323023](数据挖掘.assets/image-20230211144323023.png)

![image-20230211144339357](数据挖掘.assets/image-20230211144339357.png)

![image-20230211144350100](数据挖掘.assets/image-20230211144350100.png)

![image-20230211144414692](数据挖掘.assets/image-20230211144414692.png)

![image-20230211144442851](数据挖掘.assets/image-20230211144442851.png)

要继续决策下去，要在全是青年的数据里再进行一次ID3算法

![image-20230211145243088](数据挖掘.assets/image-20230211145243088.png)

![image-20230211145253061](数据挖掘.assets/image-20230211145253061.png)

过程略，得出的结果如下，若要再分，以此类推

![image-20230211145346196](数据挖掘.assets/image-20230211145346196.png)

## K-means聚类算法

### 课堂作业（重点）

![image-20230214175132196](数据挖掘.assets/image-20230214175132196.png)

![image-20230214175143104](数据挖掘.assets/image-20230214175143104.png)

新的聚类中心是，集合中的所有元素的均值。

当本次迭代和上次迭代出的结果相等时，迭代结束。

![image-20230214180541534](数据挖掘.assets/image-20230214180541534.png)

![image-20230214180607770](数据挖掘.assets/image-20230214180607770.png)

二维的同理

### 距离

![image-20230214175239645](数据挖掘.assets/image-20230214175239645.png)



## 分箱

![image-20230214182756135](数据挖掘.assets/image-20230214182756135.png)

等深划分：每个箱子的元素数量相等，按顺序分箱

等宽划分：每个箱子的宽度相等，箱1：[5,74)，箱2：[75,144)，箱3：[145,215]

所谓的等宽分箱就是将数据分成等宽的几份，比如模拟数据中INCOME的范围是0-150。现在将其等宽分成3份，那么每一份对应的取值范围是：`[0,50),[50,100)[100,150]`

聚类划分：对他使用k-means聚类算法吧。或者一眼分。

## 数据规范化

### 最小——最大规范化

![image-20230214185052984](数据挖掘.assets/image-20230214185052984.png)

### z-score规范化

![image-20230214185117853](数据挖掘.assets/image-20230214185117853.png)

### 小数定标规范化

![image-20230214185205113](数据挖掘.assets/image-20230214185205113.png)

### 例题

![image-20230214185322459](数据挖掘.assets/image-20230214185322459.png)

![image-20230214185406205](数据挖掘.assets/image-20230214185406205.png)

对于一组数据

![img](https://bkimg.cdn.bcebos.com/formula/1ff7bdbc205d1be096c4be1d1ad91197.svg)

 平均绝对偏差公式：m（x）为均值

![img](https://bkimg.cdn.bcebos.com/formula/97b863370ac20938025b54365e339c02.svg)

## Web数据挖掘

![image-20230214195401197](数据挖掘.assets/image-20230214195401197.png)

多类型、无规律、无结构、多噪声

![image-20230214195511707](数据挖掘.assets/image-20230214195511707.png)

![image-20230214195652121](数据挖掘.assets/image-20230214195652121.png)

## BP神经网络算法

### 激活函数

![image-20230214195916396](数据挖掘.assets/image-20230214195916396.png)

#### 阶跃函数

![image-20230214200049591](数据挖掘.assets/image-20230214200049591.png)

#### 分段线性函数

![image-20230214200113234](数据挖掘.assets/image-20230214200113234.png)

#### Sigmoid函数

![image-20230214200159849](数据挖掘.assets/image-20230214200159849.png)

![image-20230214200246714](数据挖掘.assets/image-20230214200246714.png)

#### 高斯函数

![image-20230214200302425](数据挖掘.assets/image-20230214200302425.png)

#### 初始权值

![image-20230214201427891](数据挖掘.assets/image-20230214201427891.png)

## 可视化数据类型

![image-20230214201949727](数据挖掘.assets/image-20230214201949727.png)

## 什么时候选用曼哈顿距离或欧式距离

欧式距离：相似度

曼哈顿距离：相异度

![image-20230214202639988](数据挖掘.assets/image-20230214202639988.png)

![image-20230214202331202](数据挖掘.assets/image-20230214202331202.png)